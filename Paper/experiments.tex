\section{Experiments}
Here, we discuss our experimental results on how well the methods provided in this paper work in practice. Note that evaluation of the results in practice is not an easy task as there are no real benchmark that we can test our model against and finding the distribution of the utility functions has not been explored in the literature. Furthermore, in practice, we do not know the distribution of the utility functions in advance. That is, the actual solution to our approximation of the probability distribution is unknown in practice, which renders unfeasible the evaluation of the accuracy of our method on real datasets. In addition, statistical methods provide limited tools for evaluating the probability distribution in high dimensions. 

In view of such considerations, we propose a methods to evaluate how our approach works in practice. Creating the probability distribution in our method consists of two parts. First, finding the linear utility functions for each of the users that have provided ratings and secondly, building a probability distribution from the linear utility functions. Here, we propose a procedure to evaluate these two components of our method.

We synthetically create a distribution of utility functions and from that, we sample a number of ratings for each utility function. Based on these ratings we then try to find the linear utility functions and build a probability distribution accordingly. To allow for comparisons, we make sure the original synthetic distribution contains different clusters of utility functions. Then, to evaluate the results, we compare the probability of each of the clusters. Using this method, we will be able to see whether our estimation of the probability distribution will preserve the overall shape of the distribution or not.

To be more specific, we performed two sets of experiments, one on a distribution of only linear utility functions and one on a general distribution of utility functions.  In our experiments we use two synthetically created distributions $\theta_d$ and $\theta_n$ that include $k$ clusters in $d$ and $n$ dimensions respectively, where $d$ is the dimension of a synthetically created database $D$ and $n$ is its size. $\theta_n$ is the distribution of a the utility function without any assumption on their form (linear or non-linear) while $\theta_d$ includes only linear utility functions. 

In our experiments, we vary $k$, $n$, $N$ and $d$. If a cluster $i$ is between coordinates $d_j^1$ and $d_j^2$ in the $d_j$-th dimension for all of $d_j$ values (either 1 to $d$ or 1 to $n$), we compare the probability of getting a point in those regions using our method with the original distribution. Then, we compute the mean square difference between our estimation and the original distribution to quantify the suitability of our estimation. We implemented our methods using Python programming language.

\subsection{General Original Probability Distribution}
In this section, we evaluate how well our algorithm estimates a synthetic distribution of utility functions where the distribution of the utility functions includes any form of utility functions and is not limited to only linear ones (our method still follows the linear assumption that we had, explained in previous sections). Our synthetically created distribution will include $k$ clusters, where each of the clusters follow a uniform distribution. We use a uniform distribution because in such a distribution, the boundaries of clusters are very clear. In our synthetic probability distribution, we give the $i$-th clusters the weight $\alpha_i$, for randomly selected $\alpha_i$s ($\alpha_i$s sum up to 1). We take $N$ samples from this distribution which will be the set of ratings. Furthermore, we generate $n$ random points in $d$ dimensions which constitutes our database. 

The values of $k$ (number of cluster), $d$(dimensionality of the database), $n$ (size of the database) and $N$(number of users) is set to 10, 3, 100 and 1000 respectively unless otherwise mentioned.  The result of our experiments varying $k$, $d$, $n$ and $N$ can be seen at figure 1. In each setting, we measure mean squared difference (MSD) between the result returned by our algorithm and the original distribution. The mean squared difference is calculated as follows. Each cluster $i$ in the original distribution has the weight $\alpha_i$ as mentioned above. After modeling the probability distribution using Gaussian mixture model (GMM) or kernel density estimation (KDE), we, for each cluster, take the difference  of its probability in each of the models and compare it to $\alpha_i$ which is the original probability of the cluster. We square this difference and take its mean over all the clusters which gives us the mean squared difference. The lower this measure is, the closer the shape of the estimated distribution will be to the original distribution. 

Figure 1 shows the result of our experiments. As it can be seen from the graphs, The error rate of KDE is in general lower than the GMM model. Neither of the error rates seem to be significantly affected by either of the dimension or the size of the original database or the number of user, although an increase in size of the database and the number of users results in better results by KDE, this is while GMM performs significantly worse when the dimensionality of the database is 2. Furthermore, the performance of both of the models increases when the original database has a larger number of clusters.

\subsection{Linear Original Probability Distribution}
We also performed experiments on an input of ratings that corresponds to only linear utility functions. This set of experiments allows us to evaluate the models used for building the probability distribution of the utility functions without the results being affected because of non-linearity of the original utility functions. The experiments performed here and the measure used for evaluating the results is the same  as explained above for general utility functions.

The result of the experiments in the category can be seen in Figure 2. Overall, the error rate is smaller than the previous case, as expected. However, in contrast to the previous case, the GMM model seems to be performing better than KDE model on this set of data. This can be mainly attributed to the fact that, in this case, the original clusters are on a $d$-dimensional data and we are modeling them by a $d$-dimensional model. So, if GMM can identify the clusters correctly, the only error than can occur is from approximating the original uniform distribution of the clusters by a normal distribution in the model. This is while, in the previous case, the original clusters where on an $n$-dimensional data. However, there might not be a one to one correspondence between the clusters in the $n$ dimensional data and the $d$ dimensional data. As such, finding the original clusters of the data is much harder, which curtails the performance of GMM.

Furthermore, the general trends that can be observed in the data in the set of experiments is similar to the previous case. However, there is a more observable downward trend on the errors in the case of increasing $N$.

Overall, in both sets of experiments, both of the models result in a reasonable error rate. Furthermore, the difference between the error rates from non-linear and linear utility functions is not large. This means that using a linear model for the data is adequate and is not introducing a large error margin which implies that the use of the model is justified empirically. 





